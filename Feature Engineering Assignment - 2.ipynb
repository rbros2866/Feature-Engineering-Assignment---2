{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Min-Max scaling is a data normalization technique used in data preprocessing to scale numerical features within a specific range, typically between 0 and 1. It linearly transforms the original data into a range by subtracting the minimum value from each data point and then dividing by the range of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[1000], [1500], [1200], [1800], [900]]\n",
      "\n",
      "Scaled data:\n",
      "[[0.11111111]\n",
      " [0.66666667]\n",
      " [0.33333333]\n",
      " [1.        ]\n",
      " [0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Sample data\n",
    "data = [[1000], [1500], [1200], [1800], [900]]\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the data\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nScaled data:\")\n",
    "print(scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q2.** What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Unit Vector technique, also known as vector normalization or normalization to unit norm, is a feature scaling method that scales each sample or data point to have a unit norm (length or magnitude) in the feature space. It involves dividing each data point by its magnitude, effectively scaling the vector to have a length of 1 while preserving its direction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unit vector scaling differs from Min-Max scaling primarily in how it handles the scaling process. While Min-Max scaling adjusts the values within a specific range (e.g., [0, 1]), unit vector scaling focuses on normalizing the vector so that its magnitude becomes 1 without changing the direction of the vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "[[ 3  4]\n",
      " [ 1 -2]\n",
      " [ 5  0]]\n",
      "\n",
      "Unit-scaled data:\n",
      "[[ 0.6         0.8       ]\n",
      " [ 0.4472136  -0.89442719]\n",
      " [ 1.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Sample data\n",
    "data = np.array([[3, 4], [1, -2], [5, 0]])\n",
    "\n",
    "# Calculate L2 norm for each row (axis=1)\n",
    "norms = np.linalg.norm(data, axis=1, ord=2)\n",
    "\n",
    "# Unit vector scaling\n",
    "unit_scaled_data = data / norms[:,None]\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "print(\"\\nUnit-scaled data:\")\n",
    "print(unit_scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q3.** What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal Component Analysis (PCA) is a widely used technique in machine learning and statistics for dimensionality reduction. Its main goal is to transform high-dimensional data into a lower-dimensional space while preserving the most important information or variance present in the original data.\n",
    "\n",
    "PCA works by identifying the principal components, which are new uncorrelated variables that are linear combinations of the original variables. These components are ordered by the amount of variance they explain in the data, with the first component capturing the most variance and subsequent components capturing progressively less variance.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a step-by-step explanation of PCA:\n",
    "\n",
    "Mean Centering: The mean is subtracted from each feature to ensure that the data is centered around the origin.\n",
    "\n",
    "Covariance Matrix Calculation: The covariance matrix of the mean-centered data is computed.\n",
    "\n",
    "Eigenvalue Decomposition: The eigenvectors and eigenvalues of the covariance matrix are calculated. The eigenvectors represent the directions (principal components), and the eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Selection of Principal Components: The eigenvectors corresponding to the largest eigenvalues (which explain the most variance) are chosen as the principal components.\n",
    "\n",
    "Dimensionality Reduction: The original data is projected onto the new space formed by the selected principal components, effectively reducing the dimensionality while retaining most of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed data (after PCA):\n",
      "[[-7.79422863  0.        ]\n",
      " [-2.59807621  0.        ]\n",
      " [ 2.59807621  0.        ]\n",
      " [ 7.79422863 -0.        ]]\n",
      "\n",
      "Explained variance ratio:\n",
      "[1. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([\n",
    "    [1, 2, 3],\n",
    "    [4, 5, 6],\n",
    "    [7, 8, 9],\n",
    "    [10, 11, 12]\n",
    "])\n",
    "\n",
    "# Initialize PCA and specify the number of components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit PCA on the data and transform it\n",
    "transformed_data = pca.fit_transform(data)\n",
    "\n",
    "print(\"Transformed data (after PCA):\")\n",
    "print(transformed_data)\n",
    "print(\"\\nExplained variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature\n",
    "Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA plays a crucial role in feature extraction, especially in reducing the dimensionality of the dataset by extracting or selecting the most informative features (principal components) that capture the essential information present in the original features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Relationship between PCA and Feature Extraction:**\n",
    "\n",
    "Dimensionality Reduction: PCA is a form of feature extraction that transforms high-dimensional data into a lower-dimensional space by creating new features (principal components) that are linear combinations of the original features.\n",
    "\n",
    "Information Retention: It captures the variance within the data while compressing it into a reduced set of features, thereby extracting the most important information.\n",
    "\n",
    "**Using PCA for Feature Extraction:**\n",
    "\n",
    "Identifying Important Features: PCA identifies the most important features (principal components) that explain the variance in the data.\n",
    "\n",
    "Discarding Less Important Features: It allows for discarding or compressing less relevant features that contribute minimally to the overall variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Features:\n",
      "[[ 4.47213595  0.          0.        ]\n",
      " [ 2.23606798  0.          0.        ]\n",
      " [-0.          0.          0.        ]\n",
      " [-2.23606798  0.          0.        ]\n",
      " [-4.47213595  0.         -0.        ]]\n",
      "\n",
      "Explained variance ratio:\n",
      "[1. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset (5D data)\n",
    "data = np.array([\n",
    "    [1, 2, 3, 4, 5],\n",
    "    [2, 3, 4, 5, 6],\n",
    "    [3, 4, 5, 6, 7],\n",
    "    [4, 5, 6, 7, 8],\n",
    "    [5, 6, 7, 8, 9]\n",
    "])\n",
    "\n",
    "# Initialize PCA for feature extraction\n",
    "pca = PCA(n_components=3)  # Selecting 3 principal components\n",
    "\n",
    "# Fit PCA on the data and transform it for feature extraction\n",
    "extracted_features = pca.fit_transform(data)\n",
    "\n",
    "# Print the extracted features\n",
    "print(\"Extracted Features:\")\n",
    "print(extracted_features)\n",
    "print(\"\\nExplained variance ratio:\")\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q5.** You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "Normalize numerical features like 'price,' 'rating,' and 'delivery time' to a common scale (e.g., [0, 1]) for fair comparison and modeling.\n",
    "\n",
    "**Steps for Min-Max Scaling:**\n",
    "\n",
    "**Identify Numerical Features:**\n",
    "\n",
    "Select relevant numerical features from the dataset ('price,' 'rating,' 'delivery time').\n",
    "\n",
    "**Import the Necessary Libraries:**\n",
    "\n",
    "Use libraries like scikit-learn in Python that provide Min-Max scaling functionality.\n",
    "\n",
    "**Initialize Min-Max Scaler:**\n",
    "\n",
    "Create an instance of the MinMaxScaler from the preprocessing module.\n",
    "\n",
    "**Fit and Transform:**\n",
    "\n",
    "Apply Min-Max scaling to each numerical feature separately.\n",
    "\n",
    "Calculate the minimum and maximum values for each feature.\n",
    "\n",
    "Scale the values within the range [0, 1] using the Min-Max scaling formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame({\n",
    "    'price': [1000, 1500, 1200, 1800, 900],\n",
    "    'rating': [4, 3, 5, 4, 2],\n",
    "    'delivery_time': [30, 45, 35, 50, 25]\n",
    "})\n",
    "\n",
    "numerical_columns = ['price', 'rating', 'delivery_time']\n",
    "\n",
    "# Initialize MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Fit and transform the selected columns\n",
    "data[numerical_columns] = scaler.fit_transform(data[numerical_columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rating</th>\n",
       "      <th>delivery_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      price    rating  delivery_time\n",
       "0  0.111111  0.666667            0.2\n",
       "1  0.666667  0.333333            0.8\n",
       "2  0.333333  1.000000            0.4\n",
       "3  1.000000  0.666667            1.0\n",
       "4  0.000000  0.000000            0.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outcome:\n",
    "\n",
    "The numerical features 'price,' 'rating,' and 'delivery time' will be transformed to a common scale between 0 and 1.\n",
    "Integration in Recommendation System:\n",
    "\n",
    "Utilize the scaled features as inputs for your recommendation system model.\n",
    "Algorithms like collaborative filtering or content-based filtering can utilize these scaled features for accurate recommendations.\n",
    "Refinement and Evaluation:\n",
    "\n",
    "Continuously assess the performance of the recommendation system.\n",
    "Adjust preprocessing techniques if necessary based on performance metrics and user feedback to enhance the recommendation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset Overview:**\n",
    "\n",
    "The dataset includes numerous features encompassing company-specific financial data and market trends relevant to stock price movements.\n",
    "Preprocessing Steps:\n",
    "\n",
    "**Data Cleaning:** Handle missing values, outliers, and ensure data consistency.\n",
    "\n",
    "**Normalization/Scaling:** Normalize or scale the features to bring them to a common scale, especially if they have different units or ranges.\n",
    "\n",
    "**Approach for Dimensionality Reduction:**\n",
    "\n",
    "**Feature Selection Exploration:**\n",
    "\n",
    "Consider traditional feature selection techniques (like correlation analysis or feature importance) to identify crucial features directly related to stock price prediction.\n",
    "\n",
    "**PCA Implementation:**\n",
    "\n",
    "Normalization: Scale the dataset to ensure comparable feature magnitudes.\n",
    "\n",
    "PCA Application: Apply PCA to the scaled dataset.\n",
    "\n",
    "Determine the number of principal components based on explained variance or a predefined threshold.\n",
    "\n",
    "Fit PCA to the data and transform it to a lower-dimensional space.\n",
    "\n",
    "**Model Development:**\n",
    "\n",
    "Utilize the transformed dataset post-PCA for training predictive models.\n",
    "\n",
    "Employ regression models (linear regression, SVR, etc.) or more advanced techniques tailored for stock price prediction (time series models, neural networks).\n",
    "\n",
    "**Evaluation and Refinement:**\n",
    "\n",
    "Model Evaluation: Assess model performance using relevant metrics (RMSE, MAE, etc.) using validation techniques (cross-validation, train-test split).\n",
    "\n",
    "Iterative Process: Iterate on feature selection strategies or PCA components if the model performance requires improvement.\n",
    "\n",
    "**Objective of PCA:**\n",
    "\n",
    "PCA aims to reduce dimensionality while preserving as much information/variance as possible, aiding in computational efficiency and potentially improving model accuracy in stock price prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.15598434 0.13026324 0.12116218 0.1121681  0.10047779]\n",
      "Model Score on Test Data: 0.0031885851065562854\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "data = pd.DataFrame(np.random.randn(100, 10), columns=[f'feature_{i}' for i in range(1, 11)])\n",
    "\n",
    "# Normalize or scale the data (StandardScaler in this case)\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(data)\n",
    "\n",
    "# Apply PCA for dimensionality reduction\n",
    "pca = PCA(n_components=5)  # Specify the number of components to retain\n",
    "transformed_data = pca.fit_transform(scaled_data)\n",
    "\n",
    "# Check the explained variance ratio to understand the importance of components\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "\n",
    "\n",
    "\n",
    "# Assume 'target' is the column representing stock prices in the dataset\n",
    "target = np.random.randn(100)\n",
    "\n",
    "# Split the transformed data and target into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(transformed_data, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a linear regression model\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "print(\"Model Score on Test Data:\", model.score(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q7.** For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 1  5 10 15 20]\n",
      "Scaled data (range -1 to 1): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given dataset\n",
    "data = np.array([1, 5, 10, 15, 20])\n",
    "\n",
    "min_val = -1  \n",
    "max_val = 1  \n",
    "\n",
    "# Calculate original min and max values\n",
    "data_min = np.min(data)\n",
    "data_max = np.max(data)\n",
    "\n",
    "# Min-Max scaling formula\n",
    "scaled_data = min_val + ((data - data_min) * (max_val - min_val)) / (data_max - data_min)\n",
    "\n",
    "\n",
    "print(\"Original data:\", data)\n",
    "print(\"Scaled data (range -1 to 1):\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data: [ 1  5 10 15 20]\n",
      "Scaled data (range -1 to 1): [-1.         -0.57894737 -0.05263158  0.47368421  1.        ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import numpy as np\n",
    "\n",
    "data = np.array([1, 5, 10, 15, 20]).reshape(-1, 1)  # Reshape to 2D array for MinMaxScaler\n",
    "\n",
    "# Initialize MinMaxScaler for range (-1, 1)\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Fit and transform the data to the desired range\n",
    "scaled_data = min_max_scaler.fit_transform(data)\n",
    "\n",
    "# Reshape the scaled data to a 1D array for better visibility\n",
    "scaled_data = scaled_data.flatten()\n",
    "\n",
    "print(\"Original data:\", data.flatten())\n",
    "print(\"Scaled data (range -1 to 1):\", scaled_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q8.** For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Variance Retention:**\n",
    "\n",
    "Aim to retain a high cumulative variance (e.g., 90% or more) to preserve most of the dataset's information.\n",
    "\n",
    "**Explained Variance Ratio:**\n",
    "\n",
    "Analyze the explained variance ratio provided by PCA.\n",
    "\n",
    "Retain components that collectively explain a significant portion of the variance.\n",
    "\n",
    "**Scree Plot Analysis:**\n",
    "\n",
    "Plot the explained variance against the number of components.\n",
    "\n",
    "Select the number of components before the variance explained plateaus.\n",
    "\n",
    "**Business/Application Context:**\n",
    "\n",
    "Consider the specific needs of the problem domain.\n",
    "\n",
    "Choose a number of components that balances complexity and predictive power.\n",
    "\n",
    "**Computational Efficiency:**\n",
    "\n",
    "Consider computational constraints when selecting the number of components.\n",
    "\n",
    "Fewer components reduce computational overhead.\n",
    "\n",
    "**Interpretability vs. Information Loss:**\n",
    "\n",
    "Balance interpretability and information loss.\n",
    "\n",
    "Fewer components might be more interpretable but may sacrifice some nuanced information.\n",
    "\n",
    "**Trade-off between Dimensionality and Noise:**\n",
    "\n",
    "Strive for a balance between dimensionality reduction and noise introduced by retaining too few or too many components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explained Variance Ratio: [0.26256655 0.21579693 0.20219416 0.18127308 0.13816928]\n",
      "Cumulative Explained Variance Ratio: [0.26256655 0.47836348 0.68055764 0.86183072 1.        ]\n",
      "Number of components to retain for 90% variance: 5\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "# Sample dataset or features: [height, weight, age, gender, blood pressure]\n",
    "\n",
    "# Sample data generation for demonstration\n",
    "np.random.seed(42)\n",
    "data = np.random.randn(100, 5)  # Assuming 100 samples and 5 features\n",
    "\n",
    "# Initialize PCA\n",
    "pca = PCA()\n",
    "\n",
    "# Fit PCA on the data\n",
    "pca.fit(data)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# Calculate cumulative explained variance\n",
    "cumulative_variance = np.cumsum(explained_variance_ratio)\n",
    "\n",
    "# Determine the number of components to retain (e.g., 90% variance)\n",
    "num_components = np.argmax(cumulative_variance >= 0.9) + 1\n",
    "\n",
    "# Print explained variance ratio and number of components to retain\n",
    "print(\"Explained Variance Ratio:\", explained_variance_ratio)\n",
    "print(\"Cumulative Explained Variance Ratio:\", cumulative_variance)\n",
    "print(f\"Number of components to retain for 90% variance: {num_components}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
